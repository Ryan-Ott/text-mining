{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJQVRMz2LVCa0wb4QYyuaa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ryan-Ott/text-mining/blob/master/Group20_TM_FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Mining - Final Project\n",
        "Vrije Universiteit Amsterdam, March 2023\n",
        "\n",
        "Group 20 | Antoni Stroinski, Elza Učelniece, Ryan Ott (rot280), Youssef Baccouche\n",
        "\n"
      ],
      "metadata": {
        "id": "E9mm3ea9CnbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook serves as the basis of our analysis into three core disciplines in Natural Language Processing (NLP):\n",
        "\n",
        "1.   Named Entity Recognition/Classification (NERC)\n",
        "2.   Sentiment Analysis\n",
        "3.   Topic Analysis\n",
        "\n",
        "The goal of this project is to apply the skills obtained during the Text Mining course to some common NLP tasks and to compare the performance of different systems on a given task.\n",
        "\n",
        "From data collection and inspection over model preparation and training to analysis and discussion, this work should serve as a good exercise to showcase our understanding of the complete modern NLP pipeline."
      ],
      "metadata": {
        "id": "QMmt_X7oE8Jb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis"
      ],
      "metadata": {
        "id": "SUJg_a7NHNgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "To train our model we chose to use the [sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140) dataset found on kaggle. This is a collection of 1.6 million tweets extracted straight from Twitter using the Twitter API that was used for a [Stanfort paper on Twitter Sentiment Classification](https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf). Half the tweet are of postive sentiment, the other of negative.\n",
        "\n",
        "Important to note for this dataset is that it does not contain \"gold\" standard, human generated labels but was rather machine labeled based on emoticons. If a positive emoticon was present in the tweet's text (like \":)\" or \":-D\") it was automatically labeled as positive and the emoticon itself was removed, as to make the model only learn the relationship between the written text and sentiment. The same was done for the negative class. If it contained both positive and negative emoticons it was not included as this alludes to the tweet being about different topics with differing sentiments.\n",
        "\n",
        "Important for our analysis are only the \"target\" and \"text\" columns. The target is the sentiment class: \"postive\" (initially saved as 4) or \"negative\" (initially saved as a 0). The text is the tweet itself."
      ],
      "metadata": {
        "id": "ERoHM6xSUg1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0hWshYSHsI4",
        "outputId": "2ae1b3a7-9793-4afd-9a7f-6d4119414dd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (1.5.13)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"ryanott\"\n",
        "os.environ['KAGGLE_KEY'] = \"629c69903b056199bb657b4062eb8a37\""
      ],
      "metadata": {
        "id": "XvDmRlbCHw9A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d kazanova/sentiment140"
      ],
      "metadata": {
        "id": "tJEdeGNQJZO4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2461d91b-917f-4cea-cb97-ceb7f4ce97fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading sentiment140.zip to /content\n",
            " 85% 69.0M/80.9M [00:00<00:00, 78.7MB/s]\n",
            "100% 80.9M/80.9M [00:00<00:00, 90.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip sentiment140.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqVdqPv_K_ro",
        "outputId": "db71e09d-6bdd-474e-a12d-6fc33b4b04a9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  sentiment140.zip\n",
            "  inflating: training.1600000.processed.noemoticon.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "COLUMN_NAMES = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "TWEETS_FILE = \"training.1600000.processed.noemoticon.csv\"\n",
        "\n",
        "twitter_df = pd.read_csv(TWEETS_FILE, names=COLUMN_NAMES, encoding='ISO-8859-1')\n",
        "twitter_df.drop([\"ids\", \"date\", \"flag\", \"user\"], axis=1, inplace=True)  # removing unimportant columns for our analysis\n",
        "\n",
        "twitter_df.target = twitter_df.target.apply(lambda x: \"positive\" if x == 4 else \"negative\")  # makes the target labels more human readable\n",
        "\n",
        "# Showing the label distribution\n",
        "value_counts = twitter_df['target'].value_counts()\n",
        "print(f\"Label Distribution:\\nPos: {value_counts.values[1]}\\tNeg: {value_counts.values[0]}\\n\")\n",
        "print(\"Negative examples:\\n\", twitter_df.head(3), \"\\n\")\n",
        "print(\"Positive examples:\\n\", twitter_df.tail(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-HqX0X0S0bt",
        "outputId": "f1092d69-a5aa-44e0-cc60-bcc261dadcd4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Distribution:\n",
            "Pos: 800000\tNeg: 800000\n",
            "\n",
            "Negative examples:\n",
            "      target                                               text\n",
            "0  negative  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
            "1  negative  is upset that he can't update his Facebook by ...\n",
            "2  negative  @Kenichan I dived many times for the ball. Man... \n",
            "\n",
            "Positive examples:\n",
            "            target                                               text\n",
            "1599997  positive  Are you ready for your MoJo Makeover? Ask me f...\n",
            "1599998  positive  Happy 38th Birthday to my boo of alll time!!! ...\n",
            "1599999  positive  happy #charitytuesday @theNSPCC @SparksCharity...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pre-Processing\n"
      ],
      "metadata": {
        "id": "u3BQ2-xtdtsw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XVod4_lDFMi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "nltk.download('stopwords');\n",
        "nltk.download('punkt')\n",
        "stop_words = stopwords.words('english')\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "def preprocess(text):\n",
        "  text_without_emoticons = re.sub(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', '', text)  # remove some emoticons that were still mistakenly left in the dataset\n",
        "  stripped = re.sub(r'[^\\w\\s.,]+', '', str(text_without_emoticons).lower()).strip()  # make lowercase and strip weird characters\n",
        "  \n",
        "  tokens = []\n",
        "  for token in word_tokenize(stripped):\n",
        "    if token not in stop_words:\n",
        "      tokens.append(token)\n",
        "\n",
        "  return tokens\n",
        "\n",
        "# text before and after pre-processing\n",
        "print(\"Before: \", twitter_df.loc[420, 'text'])\n",
        "twitter_df.text.apply(lambda x: preprocess(x))\n",
        "print(\"After: \", twitter_df.loc[420, 'text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IN5yTtSd3nl",
        "outputId": "4ab4e998-425b-4801-813f-cedbf73d33dd"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before:  @SupaMagg that happened to me saturday night. along with my glittery green lighter! \n",
            "After:  @SupaMagg that happened to me saturday night. along with my glittery green lighter! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_df.text[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V-8GX8we03Fq",
        "outputId": "679173f1-a50f-408b-d066-3186e9933f47"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "\n",
        "\n",
        "preprocess(twitter_df.text[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "r4dcvko__v2w",
        "outputId": "d28ce843-262b-4bfb-8292-5be43120131d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'switchfoot httptwitpic.com2y1zl  awww, thats a bummer.  you shoulda got david carr of third day to do it.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5iRPvvX1_3kC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}